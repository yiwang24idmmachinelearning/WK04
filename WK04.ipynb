{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "## Audio Processing and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/audio_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wave\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "from audio_utils import wav_to_list, list_to_wav\n",
        "from audio_utils import fft, stft, cluster_fft_freqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before we start\n",
        "\n",
        "`enumerate(list)` ...\n",
        "\n",
        "In Python, this is how we iterate over a list and get both the index AND the content for elements of the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_list = [1, 4, 6, 1, 23, 66, 6, 17]\n",
        "\n",
        "for val in my_list:\n",
        "  print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx in range(len(my_list)):\n",
        "  val = my_list[idx]\n",
        "  print(idx, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, val in enumerate(my_list):\n",
        "  print(idx, val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manipulating Audio\n",
        "\n",
        "Once we have a list of samples we can process, analyze and manipulate the audio by performing list operations and simple arithmetics.\n",
        "\n",
        "<img src=\"./imgs/audio-02.jpg\" width=\"720px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change volume\n",
        "\n",
        "To change the volume of an audio file all we have to do is multiply its samples by a constant.\n",
        "\n",
        "If the constant is greater than $1$ it will get louder, if it's between $0$ and $1$ it will get softer.\n",
        "\n",
        "<img src=\"./imgs/audio-04.jpg\" width=\"720px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the samples array to makes the audio softer and then louder\n",
        "Check results visually and by listening to the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sound_file_path = \"./data/piano.wav\"\n",
        "my_samples = wav_to_list(sound_file_path)\n",
        "\n",
        "plt.plot(my_samples)\n",
        "plt.ylim([-20000, 20000])\n",
        "plt.show()\n",
        "display(Audio(sound_file_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: make samples softer\n",
        "\n",
        "softer_samples = [s/2 for s in my_samples]\n",
        "\n",
        "plt.plot(softer_samples)\n",
        "plt.ylim([-20000, 20000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: make samples louder\n",
        "\n",
        "louder_samples = [2 * s for s in my_samples]\n",
        "plt.plot(louder_samples)\n",
        "plt.ylim([-20000, 20000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOTE: about playing modified samples\n",
        "\n",
        "The `display(Audio(samples))` function tries to be smart when it plays back our samples and automatically adjusts samples that are too loud or too soft.\n",
        "\n",
        "To hear the difference in making samples softer/louder we have to save the file and then play from the file.\n",
        "\n",
        "### Saving our samples\n",
        "\n",
        "We can use the `list_to_wav()` function to save a sequence of samples as a mono `.wav` file:\n",
        "\n",
        "```py\n",
        "list_to_wav(sum_samples, \"out.wav\")\n",
        "```\n",
        "\n",
        "### Save the modified sample lists to hear them louder and softer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: save list of softer samples as a wav file\n",
        "list_to_wav(softer_samples, \"./softer.wav\")\n",
        "\n",
        "# TODO: play from file\n",
        "display(Audio(\"./softer.wav\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: save list of louder samples as a wav file\n",
        "list_to_wav(louder_samples, \"./louder.wav\")\n",
        "\n",
        "# TODO: play from file\n",
        "display(Audio(\"./louder.wav\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change speed\n",
        "\n",
        "If we just duplicate each sample in our sequence, while keeping the sample rate the same, we'll end up with an audio file that is twice as long as the original.\n",
        "\n",
        "<img src=\"./imgs/audio-05.jpg\" width=\"720px\">\n",
        "\n",
        "And, conversely, if we remove every other sample, we'll get an audio signal that is half of the original length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the samples array to makes the audio shorter and longer\n",
        "Check results visually and by listening to the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sound_file_path = \"./data/horn.wav\"\n",
        "my_samples = wav_to_list(sound_file_path)\n",
        "\n",
        "plt.plot(my_samples)\n",
        "plt.show()\n",
        "\n",
        "display(Audio(my_samples, rate=44100))\n",
        "print(len(my_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: double the samples to hear the effects\n",
        "double_samples = []\n",
        "\n",
        "for idx, val in enumerate(my_samples):\n",
        "  double_samples.append(val)\n",
        "  double_samples.append(val)\n",
        "\n",
        "plt.plot(double_samples)\n",
        "plt.show()\n",
        "\n",
        "display(Audio(double_samples, rate=44100))\n",
        "print(len(double_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: half the samples to hear the effects\n",
        "half_samples = []\n",
        "\n",
        "for idx, val in enumerate(my_samples):\n",
        "  if idx % 2 == 0:\n",
        "    half_samples.append(val)\n",
        "\n",
        "plt.plot(half_samples)\n",
        "plt.show()\n",
        "\n",
        "display(Audio(half_samples, rate=44100))\n",
        "print(len(half_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reverse\n",
        "\n",
        "Flipping the order of the samples will make the audio sound backwards.\n",
        "\n",
        "<img src=\"./imgs/audio-06.jpg\" width=\"720px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell reverses the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sound_file_path = \"./data/two-bits.wav\"\n",
        "my_samples = wav_to_list(sound_file_path)\n",
        "\n",
        "rev_samples = list(reversed(my_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can check the effect running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(my_samples)\n",
        "plt.show()\n",
        "display(Audio(sound_file_path))\n",
        "print(my_samples[:16])\n",
        "\n",
        "plt.plot(rev_samples)\n",
        "plt.show()\n",
        "display(Audio(rev_samples, rate=44100))\n",
        "print(rev_samples[-16:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splicing\n",
        "\n",
        "We can create longer sounds by adding existing sample lists one after the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "two_bit_file_path = \"./data/two-bits.wav\"\n",
        "two_bit_samples = wav_to_list(two_bit_file_path)\n",
        "\n",
        "air_horn_file_path = \"./data/air-horn.wav\"\n",
        "air_horn_samples = wav_to_list(air_horn_file_path)\n",
        "\n",
        "plt.plot(two_bit_samples)\n",
        "plt.show()\n",
        "display(Audio(two_bit_file_path))\n",
        "print(len(two_bit_samples), \"samples\")\n",
        "\n",
        "plt.plot(air_horn_samples)\n",
        "plt.show()\n",
        "display(Audio(air_horn_file_path))\n",
        "print(len(air_horn_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sum places all of the samples from the second audio right after the first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum_samples = two_bit_samples + air_horn_samples\n",
        "\n",
        "plt.plot(sum_samples)\n",
        "plt.show()\n",
        "display(Audio(sum_samples, rate=44100))\n",
        "print(len(sum_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splicing with Slicing\n",
        "\n",
        "We can also use slicing to select parts of the two sounds before adding them.\n",
        "\n",
        "This sum keeps $60\\%$ of the first audio and then starts the second audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "end_idx = int(0.6 * len(two_bit_samples))\n",
        "\n",
        "sum_samples = two_bit_samples[:end_idx] + air_horn_samples\n",
        "\n",
        "plt.plot(sum_samples)\n",
        "plt.show()\n",
        "display(Audio(sum_samples, rate=44100))\n",
        "print(len(sum_samples), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio Analysis\n",
        "\n",
        "### Time-Domain\n",
        "\n",
        "There are a couple of simple analysis and transformations that we can perform on our samples to extract information about them and our audio signal as a whole.\n",
        "\n",
        "These are sometimes called _time-domain features_ because they are concerned with how an audio signal changes over time.\n",
        "\n",
        "Since the information we want to extract from the samples will hopefully tell us something about the audio's characteristic in terms of loudness or pitch, it's useful if we work with chunks of audio that are long enough for us to notice these properties.\n",
        "\n",
        "What this means is that we will further split our list of samples into smaller lists that contain about $10$ - $50$ milliseconds of audio.\n",
        "\n",
        "This process is sometimes called _windowing_ or _blocking_, and the result is a list of lists, where the outer list gives us a list of windows or blocks and the internal lists are just regular lists of samples:\n",
        "\n",
        "<img src=\"./imgs/window-00.jpg\" height=\"250px\">\n",
        "\n",
        "<img src=\"./imgs/window-01.jpg\" height=\"250px\">\n",
        "\n",
        "### Let's open up an audio file and split it into lists of 1024 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"./data/two-bits.wav\"\n",
        "all_samples = wav_to_list(file_path)\n",
        "\n",
        "# variable for number of samples per window, or, the window length\n",
        "WLEN = 1024\n",
        "\n",
        "# first sample index for each window: [ 0, 1024, 2048, 3072, 4096, ... ]\n",
        "wx = range(0, len(all_samples), WLEN)\n",
        "\n",
        "samples_win = []\n",
        "for s_idx in wx:\n",
        "  samples_win.append(all_samples[s_idx : s_idx + WLEN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(wx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_samples), len(samples_win)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_samples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples_win[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Root Mean Square Energy\n",
        "\n",
        "Now that we have our list split into chunks/blocks/windows, we can calculate some properties for each of these windows.\n",
        "\n",
        "The first will be a measurement of loudness called the root mean square energy. This is calculated by taking the square root of the arithmetic mean of the squares of our sample values, or:\n",
        "\n",
        "$ rms = \\sqrt{\\frac{1}{n} ({s_0}^2 + {s_1}^2 + {s_2}^2 + ... + {s_{n-1}^2})}$\n",
        "\n",
        "<img src=\"./imgs/window-02.jpg\" height=\"250px\">\n",
        "\n",
        "### Let's write a function that implements this\n",
        "\n",
        "It will receive a list of samples and return their rms value.\n",
        "\n",
        "First we can calculate the squares of all the samples with a comprehension, then find the average value of this array $\\displaystyle \\left(\\frac{sum}{length}\\right)$, and finally take the square root.\n",
        "\n",
        "Remember that in python we can take the square root of a number $x$ by raising it to $0.5$, like `x ** 0.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement rms\n",
        "def rms(samples):\n",
        "  sq_samples = [s**2 for s in samples]\n",
        "  avg_sqs = sum(sq_samples) / len(sq_samples)\n",
        "  return avg_sqs ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now, we'll use that to compute the rms for each of our windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute the rms of each window in samples_win\n",
        "\n",
        "samples_rms = [rms(samples) for samples in samples_win]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compare the length of our two arrays (`all_samples` and `samples_rms`) and also plot their contents, we'll see that even though one of them is 1000 times smaller, it's still able to represent enough information about how the loudness of the sound changes over time.\n",
        "\n",
        "This is good because if we wanted to compare it to other sounds to find similarities, instead of comparing $100,000$ values we can now compare $100$.\n",
        "\n",
        "We'll see more about this in the homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(all_samples), len(samples_rms))\n",
        "\n",
        "plt.plot(all_samples)\n",
        "plt.plot(wx, samples_rms, 'r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Crossing Rate\n",
        "\n",
        "Another time-domain feature we can extract from our samples is their zero-crossing rate, or, how frequently the wave change from a positive value to a negative one.\n",
        "\n",
        "<img src=\"./imgs/window-04.jpg\" height=\"250px\">\n",
        "\n",
        "This can give us some idea about the frequency of our sound at different points in time because higher tones, with higher frequencies, tend to have higher zero-crossing rates.\n",
        "\n",
        "The formula for computing the zero-crossing rate for a window of samples is:\n",
        "\n",
        "$\\displaystyle zcr = \\frac{1}{2} \\sum{\\left|{\\frac{|s_n|}{s_n} - \\frac{|s_{n+1}|}{s_{n+1}}} \\right|}$\n",
        "\n",
        "This looks more complicated than it should.\n",
        "\n",
        "The first thing we do is determine the sign of each sample. That's what the $\\displaystyle \\frac{|s_n|}{s_n}$ calculation does. It gives us a $+1$ if our sample is a positive number, $-1$ if it's a negative number and $0$ if the sample is $0$.\n",
        "\n",
        "Then we look at pairs of consecutive samples and subtract their signs. We'll get a $-2$ if the signal goes from a negative number to a positive number and a $+2$ if it goes from positive to negative.\n",
        "\n",
        "Finally, we sum up the absolute value of all of these $+2$ and $-2$ values and divide by $2$.\n",
        "\n",
        "### Let's write a function that implements this\n",
        "\n",
        "It will receive a list of samples and return the number of times the values change sign.\n",
        "\n",
        "We'll also implement a separate `sign()` function to do the $\\displaystyle \\frac{|s_n|}{s_n}$ calculation with a little bit of filtering to avoid counting zero-cross rates for noisy and quiet parts of of audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sign(sample):\n",
        "  if abs(sample) < 256:\n",
        "    return 0\n",
        "  else:\n",
        "    return (abs(sample) / sample)\n",
        "\n",
        "def zcr(samples):\n",
        "  signs = [sign(s) for s in samples]\n",
        "\n",
        "  twos = []\n",
        "  for i in range(0, len(samples) - 1):\n",
        "    sign_diff = signs[i] - signs[i+1]\n",
        "    twos.append(abs(sign_diff))\n",
        "\n",
        "  return (sum(twos) / 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now, we can use that to compute the zero-crossing rate for each of our windows\n",
        "\n",
        "We can then also plot this result overlaid with the original wave and rms plots.\n",
        "\n",
        "We might have to scale the `zcr()` results to make them comparable in scale to the original sample values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute the zcr of each window in samples_win \n",
        "# and plot the results along with the original wave and rms plots\n",
        "\n",
        "samples_zcr = [zcr(samples) for samples in samples_win]\n",
        "\n",
        "print(len(all_samples), len(samples_zcr))\n",
        "\n",
        "scaled_zcr = [500 * c for c in samples_zcr]\n",
        "\n",
        "plt.plot(all_samples)\n",
        "plt.plot(wx, scaled_zcr, 'r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(wx, samples_zcr, 'r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat the time-domain feature extraction for another audio file\n",
        "\n",
        "Open the file and get a list of samples, then do the windowing, the rms analysis and the zero-crossing rate calculation, and plot the results.\n",
        "\n",
        "How do they compare to the `two-bits.wav` file ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: repeat analysis with different file\n",
        "file_path = \"./data/piano.wav\"\n",
        "all_samples = wav_to_list(file_path)\n",
        "\n",
        "# first index of each window\n",
        "WLEN = 1024\n",
        "wx = range(0, len(all_samples), WLEN)\n",
        "\n",
        "samples_win = []\n",
        "\n",
        "for s_idx in wx:\n",
        "  samples_win.append(all_samples[s_idx : s_idx + WLEN])\n",
        "\n",
        "samples_rms = [rms(s) for s in samples_win]\n",
        "samples_zcr = [zcr(s) for s in samples_win]\n",
        "\n",
        "print(len(all_samples), len(samples_win))\n",
        "\n",
        "# TODO: plots\n",
        "plt.plot(all_samples)\n",
        "plt.plot(wx, samples_rms, 'r')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(wx, samples_zcr, 'g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Frequency-Domain\n",
        "\n",
        "We saw that the zero-crossing rate can sometimes tell us something about the pitch of a sound, but there's a better way to get frequency information from a sound signal.\n",
        "\n",
        "There's a mathematical operation called a Fourier Transform that we can use to decompose our audio signal into simpler, basic waves of pure frequencies.\n",
        "\n",
        "A complex audio wave made up of many frequencies:<br>\n",
        "<img src=\"./imgs/fft-00.jpg\" width=\"600px\">\n",
        "\n",
        "Gets separated into sine waves of single frequencies:<br>\n",
        "<img src=\"./imgs/fft-01.jpg\" width=\"600px\">\n",
        "\n",
        "This is useful because it can tell us which frequencies are present in our audio at any given time.\n",
        "\n",
        "The math is a bit beyond our scope here, but luckily there are many packages and libraries that implement the  Fast Fourier Transform algorithm for extracting frequency information from audio waves, and its inverse, the `IFFT`, which is used for transforming frequency information back into sound waves.\n",
        "\n",
        "### Let's open up a file, read its samples and run the fft()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"./data/two-bits.wav\"\n",
        "all_samples = wav_to_list(file_path)\n",
        "\n",
        "fft_energy, fft_freqs = fft(all_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(fft_energy), len(fft_freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_energy[:5], fft_freqs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min(fft_energy), max(fft_energy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the `fft()` on an array of samples returns two lists: one with the amount of energy in different frequency bands, and the other with the specific values of the frequency bands (in units of Hertz).\n",
        "\n",
        "We can then plot these to get information about the frequencies present in our sound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(all_samples)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(fft_freqs, fft_energy)\n",
        "plt.xlabel('Freq (Hz)')\n",
        "plt.ylabel('FFT Energy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's zoom in on the x-axis since it doesn't look like we have any frequencies less than $200$ Hz or greater than $1000$ Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(fft_freqs, fft_energy)\n",
        "plt.xlabel('Freq (Hz)')\n",
        "plt.ylabel('FFT Energy')\n",
        "plt.xlim(200, 1000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Audio(all_samples, rate=44100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can combine the two arrays that we got from `fft()` and sort them to get a list of the more prevalent frequencies.\n",
        "\n",
        "First we'll combine them using zip, then sort by the fft energy values by using a key function.\n",
        "\n",
        "We'll also round the frequencies to the nearest Hz just to make it easier to analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_energy_freq = [(round(f), e) for f,e in zip(fft_freqs, fft_energy)]\n",
        "fft_energy_freq[1800:1805]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_energy_freq[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def byFft(A):\n",
        "  return A[1]\n",
        "\n",
        "fft_sorted = sorted(fft_energy_freq, key=byFft, reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we just look at the first 5 elements of the resulting array we'll see that they all have pretty similar frequencies.\n",
        "\n",
        "Must be a very strong component of the original signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fft_sorted[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other frequencies ?\n",
        "\n",
        "Take a look at other parts of the list and see which additional frequencies are dominant in our audio signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Look into list for other frequencies\n",
        "\n",
        "fft_sorted[:15]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the top-100 strongest frequencies in a scatter plot to see how these (energy, frequency) pairs are distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_freqs = [x[0] for x in fft_sorted[:100]]\n",
        "top_energy = [x[1] for x in fft_sorted[:100]]\n",
        "plt.scatter(top_freqs, top_energy, s=8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And if we only plot the frequencies along a diagonal, we can see some pretty well-defined frequency clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(top_freqs, top_freqs, s=8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering\n",
        "\n",
        "We'll see a lot more about this in a few weeks, but this is a perfect situation where we can use a Machine Learning technique called clustering to \"learn\" how to combine similar frequencies into representative groups.\n",
        "\n",
        "The `cluster_fft_freqs()` function takes a list of fft frequency values and another list of the corresponding energy at each of those frequencies, and then calculates frequency cluster groups.\n",
        "\n",
        "There are additional optional parameters that we can use to tune this function.\n",
        "\n",
        "The `top` parameter can be used to determine how many of the top frequencies we want to use to do the clustering. The default is $50$, but since we looked at the top-100 strongest frequencies a few cells above, we can use $100$ for this parameter.\n",
        "\n",
        "Another parameter, `clusters`, can be used to specify how many groups we want to combine our data into. The default is $6$. From looking at the graphs above, maybe we can try $7$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_fft_freqs(fft_freqs, fft_energy, top=100, clusters=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat the `FFT` analysis and get the strongest $n$ frequencies in the `horn` audio file.\n",
        "\n",
        "For the horn file $n$ might be different than 7. Once we start plotting we'll see how many clusters we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: repeat FFT on piano.wav\n",
        "\n",
        "file_path = \"./data/piano.wav\"\n",
        "all_samples = wav_to_list(file_path)\n",
        "\n",
        "fft_energy, fft_freqs = fft(all_samples)\n",
        "\n",
        "fft_energy_freq = [(round(f), e) for f,e in zip(fft_freqs, fft_energy)]\n",
        "\n",
        "def byFft(A):\n",
        "  return A[1]\n",
        "\n",
        "fft_sorted = sorted(fft_energy_freq, key=byFft, reverse=True)\n",
        "\n",
        "top_freqs = [x[0] for x in fft_sorted[:100]]\n",
        "top_energy = [x[1] for x in fft_sorted[:100]]\n",
        "plt.scatter(top_freqs, top_energy, s=8)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(top_freqs, top_freqs, s=8)\n",
        "plt.show()\n",
        "\n",
        "cluster_fft_freqs(fft_freqs, fft_energy, top=100, clusters=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STFT\n",
        "\n",
        "We can run a windowed version of the `FFT` on our samples to see which frequencies are present at different times. This is called a Short-Time Fourier Transform (`STFT`) because instead of running on the entire audio at once, it runs the `FFT` on small chunks/windows of audio.\n",
        "\n",
        "Running the `stft()` on an array of samples returns three lists: one with the amount of energy in different frequency bands, at specific times, another with the specific value for the frequency bands, and a third with the specific times when the `FFT` was performed (the chunk/window time).\n",
        "\n",
        "We can plot these to get information about the frequencies present in our sound at different times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_samples = wav_to_list(\"./data/two-bits.wav\")\n",
        "\n",
        "fft_res, fft_freqs, fft_times = stft(all_samples)\n",
        "\n",
        "plt.pcolormesh(fft_times, fft_freqs, np.array(fft_res).T)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see some frequency activity on the lower frequencies.\n",
        "\n",
        "Let's zoom in on frequencies less than $2500$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.pcolormesh(fft_times, fft_freqs, np.array(fft_res).T)\n",
        "plt.ylim(0, 2500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can definitely see where each of the notes are being played and how their pitch is related to each other.\n",
        "\n",
        "For now we'll only take this quick look at the `STFT`. It can be a bit harder to use for analysis and comparisons since it has $3$ dimensions of values (time, frequency, energy), but we'll get back to it in a couple of weeks and see how it can be used in more complex Machine Learning tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
